{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do caminho para o arquivo de credenciais\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../../key.json\"\n",
    "\n",
    "# Configuração do cliente BigQuery\n",
    "client = bigquery.Client()\n",
    "gsclient = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bigquery(query: str, project_id: str = \"bankmarketingdatapipeline\", name_view: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa uma consulta no BigQuery e retorna os resultados como um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        query (str): Consulta SQL a ser executada.\n",
    "        project_id (str): ID do projeto do BigQuery.\n",
    "        name_view (Optional[str]): Nome da visualização a ser criada (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Resultados da consulta como um DataFrame.\n",
    "    \"\"\"\n",
    "    if name_view:\n",
    "        create_view(project_id, query, name_view)\n",
    "    \n",
    "    query_job = client.query(query)\n",
    "    return query_job.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_view(project_id: str, query: str, name_view: str) -> None:\n",
    "    \"\"\"\n",
    "    Cria ou substitui uma visualização no BigQuery.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): ID do projeto no BigQuery.\n",
    "        query (str): Consulta SQL a ser usada na visualização.\n",
    "        name_view (str): Nome da visualização a ser criada ou substituída.\n",
    "    \"\"\"\n",
    "    view_query = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{project_id}.{name_view}` AS {query}\n",
    "    \"\"\"\n",
    "    client.query(view_query).result()  # Executa a criação da visualização\n",
    "    print(f\"View '{name_view}' criada com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_incrementally(uri: str, table_id: str, job_config: bigquery.LoadJobConfig) -> None:\n",
    "    \"\"\"\n",
    "    Carrega dados incrementais para a tabela no BigQuery.\n",
    "    \n",
    "    Args:\n",
    "        uri (str): URI do arquivo de dados a ser carregado.\n",
    "        table_id (str): ID da tabela onde os dados serão carregados.\n",
    "        job_config (bigquery.LoadJobConfig): Configuração do job de carregamento.\n",
    "    \"\"\"\n",
    "    load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    load_job.result()  # Aguarda a conclusão do job\n",
    "    print(f\"Dados carregados incrementalmente para a tabela '{table_id}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_description(table_id: str, description: str) -> None:\n",
    "    \"\"\"\n",
    "    Atualiza a descrição da tabela no BigQuery.\n",
    "\n",
    "    Args:\n",
    "        table_id (str): ID da tabela a ser atualizada.\n",
    "        description (str): Nova descrição para a tabela.\n",
    "    \"\"\"\n",
    "    table = client.get_table(table_id)\n",
    "    table.description = description\n",
    "    client.update_table(table, [\"description\"])\n",
    "    print(f\"Descrição da tabela '{table_id}' atualizada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de carga incremental\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"order_id\", \"INTEGER\", mode=\"REQUIRED\", description=\"ID único do pedido\"),\n",
    "        bigquery.SchemaField(\"order_date\", \"TIMESTAMP\", mode=\"REQUIRED\", description=\"Data e hora do pedido\"),\n",
    "        bigquery.SchemaField(\"order_customer_id\", \"INTEGER\", mode=\"REQUIRED\", description=\"ID do cliente\"),\n",
    "        bigquery.SchemaField(\"order_status\", \"STRING\", mode=\"REQUIRED\", description=\"Status do pedido\")\n",
    "    ],\n",
    "    write_disposition=\"WRITE_APPEND\",  # Incremental: adicionar dados sem sobrescrever\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar dados incrementais no BigQuery com verificação de duplicidade\n",
    "def load_data_incrementally_with_check(uri: str, table_id: str, job_config: bigquery.LoadJobConfig):\n",
    "    # Carregar dados temporários para uma tabela temporária\n",
    "    temp_table_id = table_id + \"_temp\"\n",
    "    load_job = client.load_table_from_uri(uri, temp_table_id, job_config=job_config)\n",
    "    load_job.result()  # Aguarda a conclusão do job\n",
    "\n",
    "    print(f\"Dados carregados temporariamente para a tabela '{temp_table_id}'.\")\n",
    "\n",
    "    # Realizar o merge (evitar duplicação de 'order_id')\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE `{table_id}` AS target\n",
    "    USING `{temp_table_id}` AS source\n",
    "    ON target.order_id = source.order_id\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (order_id, order_date, order_customer_id, order_status)\n",
    "        VALUES (source.order_id, source.order_date, source.order_customer_id, source.order_status)\n",
    "    \"\"\"\n",
    "    \n",
    "    client.query(merge_query).result()  # Executa o merge\n",
    "    print(f\"Merge realizado. Dados inseridos na tabela '{table_id}'.\")\n",
    "\n",
    "    # Deletar a tabela temporária\n",
    "    client.delete_table(temp_table_id)\n",
    "    print(f\"Tabela temporária '{temp_table_id}' deletada.\")\n",
    "\n",
    "# Configuração de carga incremental (com verificação de duplicidade)\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"order_id\", \"INTEGER\", mode=\"REQUIRED\", description=\"ID único do pedido\"),\n",
    "        bigquery.SchemaField(\"order_date\", \"TIMESTAMP\", mode=\"REQUIRED\", description=\"Data e hora do pedido\"),\n",
    "        bigquery.SchemaField(\"order_customer_id\", \"INTEGER\", mode=\"REQUIRED\", description=\"ID do cliente\"),\n",
    "        bigquery.SchemaField(\"order_status\", \"STRING\", mode=\"REQUIRED\", description=\"Status do pedido\")\n",
    "    ],\n",
    "    write_disposition=\"WRITE_APPEND\",  # Incremental: adicionar dados sem sobrescrever\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: Unable to parse; line_number: 1 byte_offset_to_start_of_line: 0 column_index: 0 column_name: \"order_id\" column_type: INT64 value: \"order_id\" File: gs://raw_retail/orders/part-00008; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: CSV table references column position 3, but line contains only 1 columns.; line_number: 3 byte_offset_to_start_of_line: 93 column_index: 3 column_name: \"order_status\" column_type: STRING File: gs://raw_retail/orders/part-00008; reason: invalid, message: You are loading data without specifying data format, data will be treated as CSV format by default. If this is not what you mean, please specify data format by --source_format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m table_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbankmarketingdatapipeline.db_retail.trusted_order\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Carregar dados de forma incremental\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mload_data_incrementally\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Atualizar a descrição da tabela\u001b[39;00m\n\u001b[1;32m      8\u001b[0m update_table_description(table_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTabela contendo os pedidos confiáveis do banco de dados de varejo.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mload_data_incrementally\u001b[0;34m(uri, table_id, job_config)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCarrega dados incrementais para a tabela no BigQuery.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    job_config (bigquery.LoadJobConfig): Configuração do job de carregamento.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m load_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mload_table_from_uri(uri, table_id, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Aguarda a conclusão do job\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDados carregados incrementalmente para a tabela \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/Projetos/bank-marketing-analytics/venv/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py:969\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    968\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/Projetos/bank-marketing-analytics/venv/lib/python3.12/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: Unable to parse; line_number: 1 byte_offset_to_start_of_line: 0 column_index: 0 column_name: \"order_id\" column_type: INT64 value: \"order_id\" File: gs://raw_retail/orders/part-00008; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: CSV table references column position 3, but line contains only 1 columns.; line_number: 3 byte_offset_to_start_of_line: 93 column_index: 3 column_name: \"order_status\" column_type: STRING File: gs://raw_retail/orders/part-00008; reason: invalid, message: You are loading data without specifying data format, data will be treated as CSV format by default. If this is not what you mean, please specify data format by --source_format."
     ]
    }
   ],
   "source": [
    "uri = \"gs://raw_retail/orders/part-00008\"\n",
    "table_id = \"bankmarketingdatapipeline.db_retail.trusted_order\"\n",
    "\n",
    "# Carregar dados de forma incremental\n",
    "load_data_incrementally(uri, table_id, job_config)\n",
    "\n",
    "# Atualizar a descrição da tabela\n",
    "update_table_description(table_id, \"Tabela contendo os pedidos confiáveis do banco de dados de varejo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: Unable to parse; line_number: 1 byte_offset_to_start_of_line: 0 column_index: 0 column_name: \"order_id\" column_type: INT64 value: \"order_id\" File: gs://raw_retail/orders/part-00008; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: CSV table references column position 3, but line contains only 1 columns.; line_number: 3 byte_offset_to_start_of_line: 93 column_index: 3 column_name: \"order_status\" column_type: STRING File: gs://raw_retail/orders/part-00008; reason: invalid, message: You are loading data without specifying data format, data will be treated as CSV format by default. If this is not what you mean, please specify data format by --source_format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m table_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbankmarketingdatapipeline.db_retail.trusted_order\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Carregar dados incrementais com verificação de duplicidade\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mload_data_incrementally_with_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mload_data_incrementally_with_check\u001b[0;34m(uri, table_id, job_config)\u001b[0m\n\u001b[1;32m      4\u001b[0m temp_table_id \u001b[38;5;241m=\u001b[39m table_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_temp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m load_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mload_table_from_uri(uri, temp_table_id, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mload_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Aguarda a conclusão do job\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDados carregados temporariamente para a tabela \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Realizar o merge (evitar duplicação de 'order_id')\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/Projetos/bank-marketing-analytics/venv/lib/python3.12/site-packages/google/cloud/bigquery/job/base.py:969\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    968\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m\"\u001b[39m: retry}\n\u001b[0;32m--> 969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/Projetos/bank-marketing-analytics/venv/lib/python3.12/site-packages/google/api_core/future/polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, retry\u001b[38;5;241m=\u001b[39mretry, polling\u001b[38;5;241m=\u001b[39mpolling)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, message: Error while reading data, error message: CSV processing encountered too many errors, giving up. Rows: 2; errors: 2; max bad: 0; error percent: 0; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: Unable to parse; line_number: 1 byte_offset_to_start_of_line: 0 column_index: 0 column_name: \"order_id\" column_type: INT64 value: \"order_id\" File: gs://raw_retail/orders/part-00008; reason: invalid, location: gs://raw_retail/orders/part-00008, message: Error while reading data, error message: CSV table references column position 3, but line contains only 1 columns.; line_number: 3 byte_offset_to_start_of_line: 93 column_index: 3 column_name: \"order_status\" column_type: STRING File: gs://raw_retail/orders/part-00008; reason: invalid, message: You are loading data without specifying data format, data will be treated as CSV format by default. If this is not what you mean, please specify data format by --source_format."
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo mockado\n",
    "uri = \"gs://raw_retail/orders/part-00008\"\n",
    "table_id = \"bankmarketingdatapipeline.db_retail.trusted_order\"\n",
    "\n",
    "# Carregar dados incrementais com verificação de duplicidade\n",
    "load_data_incrementally_with_check(uri, table_id, job_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
